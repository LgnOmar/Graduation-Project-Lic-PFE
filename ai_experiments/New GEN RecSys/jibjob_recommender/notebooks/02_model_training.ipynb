{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df8b737",
   "metadata": {},
   "source": [
    "# JibJob Recommender System - Model Training\n",
    "\n",
    "This notebook demonstrates how to train the Graph Convolutional Network (GCN) recommendation model using sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = str(Path().absolute().parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from jibjob_recommender_system.config.config_loader import ConfigLoader\n",
    "from jibjob_recommender_system.data_handling.data_loader import DataLoader\n",
    "from jibjob_recommender_system.data_handling.preprocessor import DataPreprocessor\n",
    "from jibjob_recommender_system.feature_engineering.feature_orchestrator import FeatureOrchestrator\n",
    "from jibjob_recommender_system.graph_construction.graph_builder import GraphBuilder\n",
    "from jibjob_recommender_system.models.gcn_recommender import GCNRecommender\n",
    "from jibjob_recommender_system.training.train_gcn import GCNTrainer\n",
    "from jibjob_recommender_system.evaluation.evaluation import RecommendationEvaluator\n",
    "\n",
    "# Set up visualization settings\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(project_root, 'jibjob_recommender_system', 'config', 'settings.yaml')\n",
    "config = ConfigLoader.load_config(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f241585",
   "metadata": {},
   "source": [
    "## 1. Generate or Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Generate sample data\n",
    "from jibjob_recommender.sample_data.generate_sample_data import DataGenerator\n",
    "\n",
    "# Set parameters for sample data\n",
    "num_users = 100\n",
    "num_jobs = 500\n",
    "num_applications = 1000\n",
    "\n",
    "# Generate data\n",
    "data_generator = DataGenerator(seed=42)\n",
    "sample_data = data_generator.generate_sample_data(\n",
    "    num_users=num_users,\n",
    "    num_jobs=num_jobs,\n",
    "    num_applications=num_applications\n",
    ")\n",
    "\n",
    "# Extract dataframes\n",
    "users_df = sample_data['users']\n",
    "jobs_df = sample_data['jobs']\n",
    "job_applications_df = sample_data['job_applications']\n",
    "categories_df = sample_data['categories']\n",
    "\n",
    "# Create a dictionary of dataframes\n",
    "data_dict = {\n",
    "    'users': users_df,\n",
    "    'jobs': jobs_df,\n",
    "    'job_applications': job_applications_df,\n",
    "    'categories': categories_df\n",
    "}\n",
    "\n",
    "# Print data summary\n",
    "print(f\"Generated {len(users_df)} users ({len(users_df[users_df['user_type'] == 'professional'])} professionals, \"\n",
    "      f\"{len(users_df[users_df['user_type'] == 'employer'])} employers)\")\n",
    "print(f\"Generated {len(jobs_df)} jobs\")\n",
    "print(f\"Generated {len(job_applications_df)} job applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63bb04c",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = DataPreprocessor(config)\n",
    "\n",
    "# Preprocess data\n",
    "processed_data = preprocessor.preprocess(data_dict)\n",
    "\n",
    "# Display preprocessing summary\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "for key, df in processed_data.items():\n",
    "    print(f\"{key}: {len(df)} records\")\n",
    "    \n",
    "# Check for missing values\n",
    "print(\"\\nMissing values count:\")\n",
    "for key, df in processed_data.items():\n",
    "    print(f\"{key}:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a9215",
   "metadata": {},
   "source": [
    "## 3. Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature orchestrator\n",
    "feature_orchestrator = FeatureOrchestrator(config)\n",
    "\n",
    "# Generate features\n",
    "feature_data = feature_orchestrator.generate_features(processed_data)\n",
    "\n",
    "# Check embedding dimensions\n",
    "if 'users' in feature_data and 'embedding' in feature_data['users'].columns:\n",
    "    user_embedding = feature_data['users']['embedding'].iloc[0]\n",
    "    print(f\"User embedding dimension: {len(user_embedding)}\")\n",
    "    \n",
    "if 'jobs' in feature_data and 'embedding' in feature_data['jobs'].columns:\n",
    "    job_embedding = feature_data['jobs']['embedding'].iloc[0]\n",
    "    print(f\"Job embedding dimension: {len(job_embedding)}\")\n",
    "\n",
    "# Display feature data summary\n",
    "print(\"\\nFeature data summary:\")\n",
    "for key, df in feature_data.items():\n",
    "    print(f\"{key} features: {', '.join(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e6813",
   "metadata": {},
   "source": [
    "## 4. Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph builder\n",
    "graph_builder = GraphBuilder(config)\n",
    "\n",
    "# Build the graph\n",
    "graph_data = graph_builder.build_graph(feature_data)\n",
    "\n",
    "# Extract graph components\n",
    "graph = graph_data['graph']\n",
    "user_mapping = graph_data['user_mapping']\n",
    "job_mapping = graph_data['job_mapping']\n",
    "\n",
    "# Display graph information\n",
    "print(f\"Graph has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges\")\n",
    "print(f\"User nodes: {len(user_mapping)}\")\n",
    "print(f\"Job nodes: {len(job_mapping)}\")\n",
    "\n",
    "# Get node features\n",
    "node_features = graph_data['node_features']\n",
    "print(f\"Node features shape: {node_features.shape}\")\n",
    "\n",
    "# Visualize edge weight distribution\n",
    "edge_weights = [graph[u][v]['weight'] for u, v in graph.edges()]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(edge_weights, bins=20)\n",
    "plt.title('Edge Weight Distribution')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4490a553",
   "metadata": {},
   "source": [
    "## 5. Split Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "def split_data(job_applications, splits=(0.7, 0.2, 0.1), seed=42):\n",
    "    \"\"\"\n",
    "    Split job applications into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffled_idx = np.random.permutation(len(job_applications))\n",
    "    job_applications_shuffled = job_applications.iloc[shuffled_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    n = len(job_applications_shuffled)\n",
    "    train_end = int(splits[0] * n)\n",
    "    val_end = train_end + int(splits[1] * n)\n",
    "    \n",
    "    # Split the data\n",
    "    train_data = job_applications_shuffled[:train_end]\n",
    "    val_data = job_applications_shuffled[train_end:val_end]\n",
    "    test_data = job_applications_shuffled[val_end:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Create both ground truth and training data\n",
    "if 'job_applications' in data_dict:\n",
    "    train_data, val_data, test_data = split_data(data_dict['job_applications'])\n",
    "    \n",
    "    print(f\"Training set: {len(train_data)} applications\")\n",
    "    print(f\"Validation set: {len(val_data)} applications\")\n",
    "    print(f\"Test set: {len(test_data)} applications\")\n",
    "    \n",
    "    # Convert to ground truth format (user_id -> list of relevant job_ids)\n",
    "    def create_ground_truth(interactions):\n",
    "        ground_truth = {}\n",
    "        for _, row in interactions.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            job_id = row['job_id']\n",
    "            if user_id not in ground_truth:\n",
    "                ground_truth[user_id] = []\n",
    "            ground_truth[user_id].append(job_id)\n",
    "        return ground_truth\n",
    "    \n",
    "    train_ground_truth = create_ground_truth(train_data)\n",
    "    val_ground_truth = create_ground_truth(val_data)\n",
    "    test_ground_truth = create_ground_truth(test_data)\n",
    "    \n",
    "    print(f\"Training ground truth: {len(train_ground_truth)} users\")\n",
    "    print(f\"Validation ground truth: {len(val_ground_truth)} users\")\n",
    "    print(f\"Test ground truth: {len(test_ground_truth)} users\")\n",
    "else:\n",
    "    print(\"No job applications data available for splitting\")\n",
    "    # Create synthetic splits using similarity data\n",
    "    # This is a fallback if we don't have actual user-job interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dac0cd",
   "metadata": {},
   "source": [
    "## 6. Train the GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GCN model\n",
    "input_dim = node_features.shape[1]  # Feature dimension\n",
    "hidden_dims = [64, 32]  # Hidden layer dimensions\n",
    "output_dim = 32  # Output embedding dimension\n",
    "\n",
    "# Set model parameters\n",
    "model_params = {\n",
    "    'input_dim': input_dim,\n",
    "    'hidden_dims': hidden_dims,\n",
    "    'output_dim': output_dim,\n",
    "    'dropout': 0.2,\n",
    "    'use_hetero_gnn': True  # Whether to use heterogeneous GNN\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = GCNRecommender(model_params)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Created GCN model with {input_dim} input features, \"\n",
    "      f\"{hidden_dims} hidden dimensions, and {output_dim} output dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_params = {\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 5e-4,\n",
    "    'early_stopping_patience': 10,\n",
    "    'batch_size': 64,\n",
    "    'negative_samples': 5  # Number of negative samples per positive sample\n",
    "}\n",
    "\n",
    "# Create trainer\n",
    "trainer = GCNTrainer(\n",
    "    model=model,\n",
    "    graph=graph,\n",
    "    node_features=torch.FloatTensor(node_features).to(device),\n",
    "    user_mapping=user_mapping,\n",
    "    job_mapping=job_mapping,\n",
    "    train_interactions=train_data,\n",
    "    val_interactions=val_data,\n",
    "    params=training_params\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "history = trainer.train()\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for metric in ['hit_rate', 'ndcg', 'precision', 'recall']:\n",
    "    if metric in history:\n",
    "        plt.plot(history[metric], label=metric)\n",
    "plt.title('Metrics During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2d8c0",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = RecommendationEvaluator(config)\n",
    "\n",
    "# Generate recommendations for test users\n",
    "test_user_ids = list(test_ground_truth.keys())\n",
    "recommendations = {}\n",
    "top_k = 10  # Number of recommendations to generate\n",
    "\n",
    "for user_id in test_user_ids:\n",
    "    # Skip user if not in mapping\n",
    "    if user_id not in user_mapping:\n",
    "        continue\n",
    "        \n",
    "    # Get user node index\n",
    "    user_idx = user_mapping[user_id]\n",
    "    \n",
    "    # Get user embedding (forward pass through the trained model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_embeddings = model(graph, torch.FloatTensor(node_features).to(device))\n",
    "        user_emb = node_embeddings[user_idx]\n",
    "        \n",
    "        # Calculate scores for all jobs\n",
    "        job_scores = []\n",
    "        for job_id, job_idx in job_mapping.items():\n",
    "            job_emb = node_embeddings[job_idx]\n",
    "            score = torch.dot(user_emb, job_emb).item()\n",
    "            job_scores.append((job_id, score))\n",
    "            \n",
    "        # Sort by score and take top k\n",
    "        job_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        recommendations[user_id] = [js[0] for js in job_scores[:top_k]]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"\\nEvaluation Results:\")\n",
    "results = evaluator.evaluate_all_metrics(\n",
    "    recommendations=recommendations,\n",
    "    ground_truth=test_ground_truth,\n",
    "    job_categories={job_id: job_row['categories'] for job_id, job_row in feature_data['jobs'].iterrows()},\n",
    "    all_items=feature_data['jobs'].index.tolist()\n",
    ")\n",
    "\n",
    "# Display metrics for each k value\n",
    "for metric, values in results.items():\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    for k, value in values.items():\n",
    "        print(f\"  @{k}: {value:.4f}\")\n",
    "        \n",
    "# Visualize key metrics at different k values\n",
    "plt.figure(figsize=(14, 8))\n",
    "metrics = ['hit_rate', 'precision', 'recall', 'ndcg', 'map']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    if metric in results:\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        ks = list(results[metric].keys())\n",
    "        values = list(results[metric].values())\n",
    "        plt.plot(ks, values, 'o-')\n",
    "        plt.title(f'{metric.upper()}')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4c5c7",
   "metadata": {},
   "source": [
    "## 8. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = os.path.join(project_root, 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = os.path.join(models_dir, f\"gcn_model_{timestamp}.pt\")\n",
    "\n",
    "# Prepare model data for saving\n",
    "model_data = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_params': model_params,\n",
    "    'user_mapping': user_mapping,\n",
    "    'job_mapping': job_mapping,\n",
    "    'input_dim': input_dim,\n",
    "    'timestamp': timestamp,\n",
    "    'config': config\n",
    "}\n",
    "\n",
    "torch.save(model_data, model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcb83f",
   "metadata": {},
   "source": [
    "## 9. Generate Sample Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea29468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few sample users\n",
    "sample_user_ids = list(user_mapping.keys())[:5]\n",
    "\n",
    "# Generate recommendations for sample users\n",
    "sample_recommendations = {}\n",
    "top_k = 10\n",
    "\n",
    "for user_id in sample_user_ids:\n",
    "    user_idx = user_mapping[user_id]\n",
    "    \n",
    "    # Get user embedding\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_embeddings = model(graph, torch.FloatTensor(node_features).to(device))\n",
    "        user_emb = node_embeddings[user_idx]\n",
    "        \n",
    "        # Calculate scores for all jobs\n",
    "        job_scores = []\n",
    "        for job_id, job_idx in job_mapping.items():\n",
    "            job_emb = node_embeddings[job_idx]\n",
    "            score = torch.dot(user_emb, job_emb).item()\n",
    "            job_scores.append((job_id, score))\n",
    "            \n",
    "        # Sort by score and take top k\n",
    "        job_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_jobs = job_scores[:top_k]\n",
    "        \n",
    "        # Get job details\n",
    "        recommendations_with_details = []\n",
    "        for job_id, score in top_jobs:\n",
    "            if job_id in feature_data['jobs'].index:\n",
    "                job_info = feature_data['jobs'].loc[job_id]\n",
    "                recommendations_with_details.append({\n",
    "                    'job_id': job_id,\n",
    "                    'score': score,\n",
    "                    'categories': job_info.get('categories', []),\n",
    "                    'title': job_info.get('title', 'No title available')\n",
    "                })\n",
    "                \n",
    "        sample_recommendations[user_id] = recommendations_with_details\n",
    "\n",
    "# Display recommendations for each sample user\n",
    "for user_id, recs in sample_recommendations.items():\n",
    "    # Get user details\n",
    "    user_info = feature_data['users'].loc[user_id] if user_id in feature_data['users'].index else {}\n",
    "    user_categories = user_info.get('categories', [])\n",
    "    \n",
    "    print(f\"\\n=== Recommendations for User {user_id} ===\")\n",
    "    print(f\"User Categories: {', '.join(user_categories)}\")\n",
    "    print(\"\\nTop Recommended Jobs:\")\n",
    "    for i, rec in enumerate(recs, 1):\n",
    "        print(f\"{i}. {rec['title']} (ID: {rec['job_id']})\")\n",
    "        print(f\"   Score: {rec['score']:.4f}\")\n",
    "        print(f\"   Categories: {', '.join(rec['categories'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a1d90",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to:\n",
    "\n",
    "1. Generate or load sample data for the JibJob recommendation system\n",
    "2. Preprocess the data and generate features\n",
    "3. Build a graph representation of users and jobs\n",
    "4. Train a GCN model on the graph data\n",
    "5. Evaluate the model using various recommendation metrics\n",
    "6. Generate and display sample recommendations\n",
    "\n",
    "This approach can be extended to real-world data by replacing the sample data generation with actual data from the JibJob platform. The same model architecture and training process would apply."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

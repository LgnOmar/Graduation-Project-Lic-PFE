GLOBAL PROMPT FOR AI AGENT: JIBJOB RECOMMENDATION SYSTEM DEVELOPMENT
PREAMBLE: STRICT ADHERENCE REQUIRED
You are an AI Agent tasked with the complete end-to-end development of a sophisticated job recommendation system for the JibJob platform. Your output must be a fully functional, production-ready (or near-production-ready) codebase, accompanied by comprehensive documentation. This prompt is designed to be extremely super structured, incredibly detailed, blunt, and honest. Deviations from these instructions without explicit justification and pre-approval (hypothetically) will be considered a failure. You must focus on identifying and mitigating potential flaws, especially logical inconsistencies, at every stage. Your approach must be productive, efficient, and technically sound.
I. OVERARCHING GOAL & CONTEXT
1.	Primary Objective: Develop a recommendation system that accurately and efficiently recommends relevant job/gig offers to "Professional" users on the JibJob platform.
2.	Platform Overview:
o	JibJob: A platform connecting individuals posting small jobs/gigs ("Clients") with individuals seeking such work ("Professionals").
o	User Types:
	Client: Posts job offers (title, description, location). Does not select work categories for themselves.
	Professional: Seeks jobs. Has a profile with selected work categories (e.g., Tutoring, Mechanic, Electrician) and a profile bio.
o	Location Data: Available for both user types and jobs (textual and numerical Latitude/Longitude). Distance calculation is a key factor.
3.	Core Recommendation Logic: Recommendations are solely for Professionals, suggesting suitable jobs. An electrician should primarily see electrician jobs, considering location and other factors.
4.	Mandatory Technologies:
o	Graph Convolutional Networks (GCNs) and/or Heterogeneous Graph Convolutional Networks (HeteroGCNs).
o	Word Embeddings using BERT (or a similar transformer-based model like DistilBERT for efficiency, but justify) for textual features.
II. CORE REQUIREMENTS & CONSTRAINTS
1.	Dataset Handling:
o	The dataset will be provided synthetically and independently. Assume it resides in the project's root directory or a subdirectory named sample_data/.
o	You MUST design your system to consume data adhering precisely to the schema defined in Section III.
o	Implement robust data validation upon loading. Log errors and handle missing/corrupt data gracefully (e.g., skip problematic records with warnings, do not crash).
2.	Accuracy & Relevance: The system must prioritize recommending jobs that are highly relevant to a professional's selected categories, skills (inferred from bio), and location.
3.	Scalability (Consideration): While the initial dataset might be synthetic/small, design your data structures, algorithms, and code with considerations for future scalability. For instance, avoid N+1 query patterns if interacting with a hypothetical database representation of this data. Use efficient data structures (e.g., Pandas DataFrames optimized, NumPy arrays).
4.	Modularity: Code MUST be highly modular. Separate concerns into distinct Python modules/packages (data processing, feature engineering, model definition, training, inference, utilities).
5.	Configuration: Critical parameters (e.g., model hyperparameters, file paths, embedding dimensions) MUST be configurable, ideally through a central configuration file (e.g., YAML or JSON) or environment variables.
6.	Error Handling & Logging: Implement comprehensive error handling and detailed logging (using Python's logging module) throughout the system. Logs should be informative for debugging and monitoring.
7.	Reproducibility: Ensure that experiments and model training are reproducible. This includes setting random seeds (NumPy, PyTorch/TensorFlow, Python's random).
8.	Flaw Mitigation: Proactively identify and address potential logical flaws, data inconsistencies, or algorithmic biases. For example:
o	Cold Start: How will the system recommend jobs to a new professional with no interaction history? (Hint: Rely heavily on profile categories, bio, and location). How will it handle new jobs with no applications yet?
o	Popularity Bias: How will you prevent the system from only recommending extremely popular jobs and ensure discoverability for less popular but relevant jobs?
o	Data Sparsity: Acknowledge this, especially in interaction data (e.g., job_applications.csv). GCNs can help, but initial feature engineering is critical.
III. DETAILED DATA STRUCTURE SPECIFICATION (SYNTHETIC DATA INPUT)
Your system MUST expect data in the following formats and schemas. Data will be located in a ./sample_data/ directory.
1.	locations.json: A JSON array of location objects.
o	Schema:
o	      [
o	    {
o	        "location_id": "loc_001",       // STRING: Unique identifier for the location
o	        "post_code": "01001",           // STRING: Postal code
o	        "name": "Adrar",                // STRING: City/Administrative area name
o	        "wilaya_name": "Adrar",         // STRING: Larger administrative region (e.g., state, province)
o	        "longitude": -0.4841573,        // FLOAT: Longitude
o	        "latitude": 27.9763317          // FLOAT: Latitude
o	    }
o	    // ... more location objects
o	]
    
o	Constraint: location_id must be unique. Longitude and Latitude must be valid floating-point numbers.
2.	categories.csv: CSV file listing all possible job categories.
o	Columns:
	category_id (STRING, Primary Key): Unique identifier for the category (e.g., "cat_elec", "cat_plum").
	category_name (STRING): Human-readable name of the category (e.g., "Electrician", "Plumber").
o	Example:
o	      category_id,category_name
o	cat_001,Tutoring
o	cat_002,Mechanic
o	cat_003,Electrician
    Constraint: category_id must be unique.
3.	users.csv: CSV file containing information for both "Client" and "Professional" users.
o	Columns:
	user_id (STRING, Primary Key): Unique identifier for the user (e.g., "prof_001", "client_001"). Prefixes can help distinguish types but are not mandatory for schema, user_type field handles this.
	username (STRING): Display name of the user.
	user_type (STRING): Enum: "professional" or "client".
	location_id (STRING, Foreign Key): References location_id in locations.json.
	profile_bio (STRING): Textual bio. For "professional" users, this is crucial for skill inference. For "client" users, this may be empty or contain organizational info.
o	Example:
o	      user_id,username,user_type,location_id,profile_bio
o	prof_001,pro_user_A,professional,loc_001,"Experienced electrician specializing in residential wiring and smart home installations. Certified for 10 years."
o	client_001,client_user_X,client,loc_002,"Homeowner looking for occasional help with garden and house maintenance."
o	prof_002,pro_user_B,professional,loc_003,"Mathematics and Physics tutor for high school and early college students. M.Sc. in Applied Physics."
    
Constraint: user_id must be unique. user_type must be one of the specified values. location_id must exist in locations.json.
4.	professional_categories.csv: CSV file mapping Professionals to their selected skill categories. (Normalization of "selected_category_ids").
o	Columns:
	user_id (STRING, Foreign Key): References user_id for a "professional" type user in users.csv.
	category_id (STRING, Foreign Key): References category_id in categories.csv.
o	Example:
o	      user_id,category_id
o	prof_001,cat_003  // prof_001 is an Electrician
o	prof_002,cat_001  // prof_002 is a Tutor
o	prof_001,cat_00X  // prof_001 might also be, e.g., a "Smart Home Installer" (new category)
    
Constraint: The combination of user_id and category_id should ideally be unique. user_id must reference a user with user_type = "professional". category_id must exist in categories.csv.
5.	jobs.csv: CSV file containing job/gig postings.
o	Columns:
	job_id (STRING, Primary Key): Unique identifier for the job.
	title (STRING): Job title.
	description (STRING): Detailed job description. This is CRUCIAL for BERT embeddings.
	location_id (STRING, Foreign Key): References location_id in locations.json. The physical location where the job needs to be done.
	posted_by_user_id (STRING, Foreign Key): References user_id for a "client" type user in users.csv.
	required_category_id (STRING, Foreign Key, Potentially Nullable): References category_id in categories.csv. This is the primary category the client believes the job falls into. If clients can select multiple, this synthetic data might provide the most relevant single category. The system can also infer categories from title/description. For a robust first version, assume this is present and singular.
o	Example:
o	      job_id,title,description,location_id,posted_by_user_id,required_category_id
o	job_001,"Urgent: Licensed Electrician for New Room Wiring","Need a certified electrician for complete wiring of a newly constructed room (approx 15sqm). Includes power outlets, lighting fixtures. Materials will be discussed. Must provide proof of license. Job in Adrar city center.",loc_001,client_001,cat_003
o	job_002,"Math Tutor for Grade 10 Student","Seeking an experienced math tutor for my son in Grade 10. Focus on Algebra and Geometry. 2 sessions per week, 2 hours each. Preferably in-person at our home in Oran.",loc_004,client_002,cat_001
    
Constraint: job_id must be unique. location_id must exist. posted_by_user_id must reference a "client" user. required_category_id (if not nullable) must exist in categories.csv.
6.	job_applications.csv (OPTIONAL BUT HIGHLY RECOMMENDED FOR GCN EFFICACY): CSV file representing historical interactions (Professionals applying to Jobs).
o	If this file is NOT provided with the initial synthetic dataset, your system should still be designed to potentially leverage it in the future (e.g., by allowing the GCN to incorporate such edges if the data becomes available). For the initial implementation, if this is absent, the GCN will rely more on content and profile similarities. STATE CLEARLY in your design if you assume its absence and how that impacts GCN strategy.
o	Columns:
	application_id (STRING, Primary Key): Unique ID for the application event.
	job_id (STRING, Foreign Key): References job_id in jobs.csv.
	professional_user_id (STRING, Foreign Key): References user_id for a "professional" in users.csv.
	application_status (STRING): e.g., "applied", "viewed", "accepted", "rejected". For recommendation, "applied" or any positive interaction is key.
	timestamp (DATETIMEISO8601 STRING): e.g., "2023-10-26T10:00:00Z".
o	Example:
o	      application_id,job_id,professional_user_id,application_status,timestamp
o	app_001,job_001,prof_001,applied,2023-10-26T10:00:00Z
o	app_002,job_002,prof_002,applied,2023-10-27T12:30:00Z
    
Constraint: application_id unique. job_id exists. professional_user_id references a "professional" user.
IV. DETAILED PROJECT STRUCTURE & FILE ORGANIZATION
You MUST organize the project using the following structure. Create empty __init__.py files in directories to make them Python packages.
      jibjob_recommender/
│
├── sample_data/                   # Location for input CSV/JSON files (as per Section III)
│   ├── locations.json
│   ├── categories.csv
│   ├── users.csv
│   ├── professional_categories.csv
│   ├── jobs.csv
│   └── job_applications.csv       # (If provided)
│
├── jibjob_recommender_system/     # Main application package
│   │
│   ├── __init__.py
│   │
│   ├── config/                    # Configuration files and parsers
│   │   ├── __init__.py
│   │   ├── settings.yaml          # Main configuration (paths, hyperparameters)
│   │   └── config_loader.py       # Logic to load and validate settings.yaml
│   │
│   ├── data_handling/             # Data loading, validation, preprocessing
│   │   ├── __init__.py
│   │   ├── data_loader.py         # Loads all raw data into appropriate structures (e.g., Pandas DFs)
│   │   └── data_validator.py      # Validates schema and integrity of loaded data
│   │   └── preprocessor.py        # Text cleaning, numerical conversions, etc.
│   │
│   ├── feature_engineering/       # Creating features for models
│   │   ├── __init__.py
│   │   ├── text_embedder.py       # BERT (or chosen model) embedding generation
│   │   ├── location_features.py   # Distance calculation, location encoding
│   │   ├── graph_features.py      # Prepare node/edge features for GCN
│   │   └── feature_orchestrator.py # Combines various features for model input
│   │
│   ├── graph_construction/        # Logic to build the graph for GCNs
│   │   ├── __init__.py
│   │   ├── graph_builder.py       # Constructs the graph (homogeneous or heterogeneous) using libraries like PyG or DGL
│   │   └── heterogeneous_graph_def.py # (If HeteroGCN) Defines node types, edge types, meta-paths
│   │
│   ├── models/                    # Model definitions
│   │   ├── __init__.py
│   │   ├── base_recommender.py    # Abstract base class for recommenders (optional, for structure)
│   │   ├── gcn_recommender.py     # GCN/HeteroGCN model architecture (PyTorch/TensorFlow)
│   │   └── (other_models.py)      # For any baseline or alternative models for comparison
│   │
│   ├── training/                  # Training scripts and utilities
│   │   ├── __init__.py
│   │   ├── trainer.py             # Main training loop, loss functions, optimizers
│   │   ├── evaluation.py          # Metrics calculation (P@K, R@K, NDCG@K)
│   │   └── train_gcn.py           # Script to orchestrate training of GCN model
│   │
│   ├── inference/                 # Logic for generating recommendations
│   │   ├── __init__.py
│   │   ├── recommender_service.py # Class/functions to take a professional_user_id and return N job_ids
│   │   └── predict.py             # Example script/entry point for getting recommendations
│   │
│   ├── utils/                     # Common utility functions
│   │   ├── __init__.py
│   │   ├── helpers.py             # Generic helper functions (e.g., distance calc, file ops)
│   │   └── logging_config.py      # Configure logging setup
│   │
│   └── main.py                    # Main script to run end-to-end process (e.g., data prep -> train -> eval -> save model)
│
├── notebooks/                     # Jupyter notebooks for exploration, analysis, visualization (OPTIONAL BUT RECOMMENDED)
│   ├── 01_data_exploration.ipynb
│   ├── 02_feature_engineering_eda.ipynb
│   ├── 03_model_prototyping.ipynb
│
├── tests/                         # Unit and integration tests
│   ├── __init__.py
│   ├── test_data_handling.py
│   ├── test_feature_engineering.py
│   ├── test_models.py
│   └── (more test files matching modules)
│
├── requirements.txt               # Python package dependencies
├── README.md                      # Detailed project documentation
├── .gitignore                     # Git ignore file
└── (other root level configs like Dockerfile, pylintrc, etc. if applicable)
    

V. STEP-BY-STEP DEVELOPMENT PLAN
Execute the following steps meticulously. Each step must result in functional, well-documented, and tested code (where applicable).
Phase 1: Foundation & Data Processing
1.	Task 1: Environment Setup & Project Scaffolding
o	Sub-Task 1.1: Create the directory structure outlined in Section IV.
o	Sub-Task 1.2: Initialize __init__.py files.
o	Sub-Task 1.3: Set up a virtual environment (e.g., venv, conda).
o	Sub-Task 1.4: Create requirements.txt. Initial entries: pandas, numpy, scikit-learn, transformers, torch (or tensorflow), pyyaml (for config), graph library (e.g., torch_geometric, dgl). Specify versions.
o	Sub-Task 1.5: Implement config/config_loader.py and create a sample settings.yaml defining paths to sample_data/.
o	Sub-Task 1.6: Implement utils/logging_config.py.
2.	Task 2: Data Loading and Validation Module (data_handling)
o	Sub-Task 2.1: Implement data_loader.py:
	Functions to load locations.json. Handle potential JSON parsing errors.
	Functions to load each CSV file (categories.csv, users.csv, professional_categories.csv, jobs.csv, job_applications.csv if present) into Pandas DataFrames.
	Handle file not found errors gracefully.
	Ensure correct data types are inferred or explicitly cast (e.g., longitude, latitude as float).
o	Sub-Task 2.2: Implement data_validator.py:
	Functions to validate loaded DataFrames against expected schemas (column names, non-null constraints where critical).
	Validate foreign key relationships (e.g., location_id in users.csv exists in loaded locations data). Log inconsistencies. CRITICAL: Do not proceed with invalid data that breaks fundamental assumptions.
	Validate user_type enum values.
o	Sub-Task 2.3: Implement basic preprocessing in preprocessor.py:
	Text cleaning for profile_bio, title, description (e.g., lowercase, remove special characters, handle HTML if present – though synthetic data shouldn't have it).
	Date parsing for timestamp in job_applications.csv (if used).
o	Unit Tests: Write tests for loading different scenarios (correct data, missing files, malformed data).
Phase 2: Feature Engineering
1.	Task 3: Feature Engineering Module (feature_engineering)
o	Sub-Task 3.1: Location Features (location_features.py)
	Implement Haversine distance calculation: calculate_distance(lat1, lon1, lat2, lon2) -> distance in km.
	Create functions to enrich job and professional data with distances. The actual pairing for distance (Professional_X vs Job_Y) will occur at recommendation time or during graph edge creation. Store processed location data effectively.
o	Sub-Task 3.2: Text Embedding (text_embedder.py)
	Integrate a pre-trained BERT model (e.g., bert-base-uncased or DistilBERT via transformers library).
	Function to generate embeddings for:
	Job titles + descriptions (concatenate or process separately and then combine).
	Professional profile_bio.
	Store these embeddings efficiently, associated with their respective job_id or user_id. Consider saving to disk (e.g., .npy files or a feature store format) if generation is time-consuming for large datasets.
	CRITICAL: Handle tokenization, padding, and attention masks correctly. Use appropriate pooling strategy (e.g., CLS token, mean pooling) to get sentence-level embeddings.
o	Sub-Task 3.3: Graph Features (graph_features.py)
	Define how node features will be constructed.
	Professional Node Features: Combine BERT embedding of profile_bio, one-hot encoded selected categories (or embeddings of category names), location features (raw lat/long or an embedding of location).
	Job Node Features: Combine BERT embedding of title/description, one-hot encoded required_category_id (or embedding of category name), location features.
	Category Node Features: Potentially embeddings of category_name (e.g., using a simple Word2Vec or averaging BERT embeddings of jobs/professionals in that category). Or simply use them as identifiers if linking via edges.
	Location Node Features: Raw lat/long or learnable embeddings if location is a node type.
	Define edge features if applicable (e.g., distance for Professional-Job location proximity edge).
o	Sub-Task 3.4: Feature Orchestration (feature_orchestrator.py)
	A script/class that calls the above modules to generate and save all necessary features.
o	Unit Tests: Test distance calculation, embedding generation (shape, type), feature combination logic.
Phase 3: Graph Construction & Modeling
1.	Task 4: Graph Construction Module (graph_construction)
o	Sub-Task 4.1: Define Graph Structure (heterogeneous_graph_def.py - if using HeteroGCN, else conceptualize for GCN)
	Node Types: Clearly define: Professional, Job, Category. Optionally Location, Client. Justify choices.
	Edge Types (Meta-paths for HeteroGCN):
	(Professional) -[INTERESTED_IN]-> (Category) (from professional_categories.csv)
	(Job) -[REQUIRES_CATEGORY]-> (Category) (from jobs.csv.required_category_id)
	(Job) -[LOCATED_AT]-> (Location) (from jobs.csv.location_id)
	(Professional) -[RESIDES_IN]-> (Location) (from users.csv.location_id)
	(If job_applications.csv is used): (Professional) -[APPLIED_TO]-> (Job)
	(Advanced - Content Similarity Edges):
	(Professional) -[SIMILAR_PROFILE_TO]-> (Professional) (based on cosine similarity of profile_bio BERT embeddings above a threshold).
	(Job) -[SIMILAR_DESCRIPTION_TO]-> (Job) (based on cosine similarity of job description BERT embeddings above a threshold).
	(Contextual Edge): (Job) -[POSTED_BY]-> (Client)
	Decision Point: Homogeneous vs. Heterogeneous GCN:
	Heterogeneous GCN (Recommended): Allows distinct node/edge types and tailored message passing. Use PyG's HeteroConv or DGL's equivalent. This is preferred given the diverse entities.
	Homogeneous GCN: Simpler, but might lose semantic richness. All nodes/edges treated similarly, distinct information often packed into node features. If chosen, justify why simplification is preferred.
o	Sub-Task 4.2: Implement graph_builder.py
	Function to take processed DataFrames and feature files as input.
	Construct the graph object using PyTorch Geometric (torch_geometric.data.Data or HeteroData) or DGL (dgl.DGLGraph).
	Map user_id, job_id, category_id etc. to integer node indices for the graph library. Maintain these mappings.
	Assign node features (from Task 3.3) to the respective nodes.
	Create edges based on the defined relations. Edge attributes (like distance, or interaction type) can also be added if the GCN architecture supports them.
o	Critical consideration: How to handle professionals and jobs not connected to any categories? They might become isolated nodes. This speaks to the importance of required_category_id in jobs.csv and robust category selection for professionals.
2.	Task 5: Model Implementation (models)
o	Sub-Task 5.1: Implement gcn_recommender.py
	Define the GCN/HeteroGCN architecture (using PyTorch or TensorFlow).
	For HeteroGCN: Define different GNNConv layers for different edge types or use a wrapper like HeteroConv that applies specified convolutions per edge type.
	Input: Graph data, node features.
	Output: Embeddings for Professional nodes and Job nodes.
	The model should learn to project Professionals and Jobs into a common embedding space where proximity indicates relevance.
	Link Prediction Task: The typical GCN training setup for recommendations is often framed as link prediction (e.g., predicting APPLIED_TO edge or a general "is_relevant" edge). The dot product or a small MLP can be used on pairs of (Professional, Job) embeddings to predict this.
o	Consider Model Layers: Typical GCN layers (GCNConv, GATConv, SAGEConv). For HeteroGCN, RGCNConv or custom HeteroConv stacks.
o	Parameters: Number of layers, hidden dimensions, activation functions, dropout. Make these configurable.
Phase 4: Training, Evaluation, and Inference
1.	Task 6: Training & Evaluation Module (training)
o	Sub-Task 6.1: Implement trainer.py
	Data Splitting: Devise a strategy for train/validation/test splits. If using interaction data (job_applications.csv), time-based splitting is preferred to simulate real-world scenarios (train on past, test on future). If no interaction data, this becomes trickier. You might train to predict affinity based on profile/job features, where "ground truth" pairs are (Professional, Job in their category & nearby). Define this "ground truth" or proxy task carefully.
	Negative Sampling: For link prediction, you'll need negative samples (pairs of Professional-Job that are not relevant or not applied to). Implement a negative sampling strategy (e.g., random J-P_not_interacted, or random J-P_not_in_category). This is CRITICAL.
	Loss Function: Binary Cross-Entropy (BCE) for link prediction is common. Margin-based losses (e.g., Triplet Loss, BPR Loss) can also be effective.
	Optimizer: Adam, AdamW.
	Training Loop: Iterate over epochs, process batches of data (positive/negative pairs or subgraphs if using graph sampling techniques like GraphSAINT or ClusterGCN for scalability, though perhaps overkill for initial synthetic dataset size).
	Model Checkpointing: Save the best model based on validation performance.
o	Sub-Task 6.2: Implement evaluation.py
	Metrics: Precision@K, Recall@K, NDCG@K.
	Evaluation Protocol: For each Professional in the validation/test set, use the trained model to score all (or a large sample of) candidate Jobs. Rank jobs by score. Compare top K recommended jobs against actual relevant jobs (ground truth for that Professional).
	Ground Truth for Evaluation: This is critical.
	If job_applications.csv exists: Held-out actual applications are the ground truth.
	If not: A relevant job for Professional P could be defined as a Job J such that:
1.	J's required_category_id is among P's professional_categories.
2.	Distance(P's location, J's location) < Threshold_Distance.
Carefully define this heuristic.
o	Sub-Task 6.3: Implement train_gcn.py
	Orchestration script that:
	Loads data and features.
	Builds the graph.
	Initializes the model, optimizer, loss function.
	Runs the training loop using trainer.py.
	Performs evaluation using evaluation.py.
	Saves the trained model and evaluation results.
2.	Task 7: Inference/Recommendation Service Module (inference)
o	Sub-Task 7.1: Implement recommender_service.py
	Function get_recommendations(professional_user_id, top_n, model, graph_data, all_job_data):
1.	Load the trained GCN model.
2.	Get the embedding for the given professional_user_id.
3.	Get embeddings for all candidate Job nodes.
4.	Calculate similarity scores (e.g., dot product) between the professional's embedding and all job embeddings.
5.	Filtering (Post-GCN):
	Hard Filter by Category (optional but recommended): Initially only consider jobs whose required_category_id matches one of the professional's categories. The GCN might learn this, but explicit filtering can enforce business rules.
	Hard Filter by Location: Exclude jobs beyond a maximum acceptable distance (this should be configurable). This MUST be applied. The GCN might learn location preference, but a hard cutoff is pragmatic.
6.	Rank the filtered jobs by similarity score.
7.	Return the top top_n job_ids.
o	Sub-Task 7.2: Implement predict.py
	A command-line script or simple interface to test recommender_service.py. Example: python -m jibjob_recommender_system.inference.predict --user_id prof_001 --top_n 10.
Phase 5: Utilities, Testing, and Documentation
1.	Task 8: Utility Module (utils)
o	Sub-Task 8.1: Implement helpers.py:
	Consolidate generic utility functions (e.g., detailed Haversine distance, any complex data transformations not fitting elsewhere, saving/loading Python objects).
2.	Task 9: Testing (Unit & Integration) (tests)
o	Sub-Task 9.1: Write unit tests for all critical functions in each module.
	test_data_handling.py: Test loading, validation, preprocessing with mock data.
	test_feature_engineering.py: Test embedding output shapes, distance calculations.
	test_graph_construction.py: Test graph object properties (num_nodes, num_edges for different types).
	test_models.py: Test model forward pass, output shapes.
	test_training.py: Test parts of the training loop (e.g., loss calculation on a tiny batch).
	test_inference.py: Test recommendation output format.
o	Sub-Task 9.2 (Optional but good): Write a few integration tests to check the flow between modules (e.g., data loading -> feature engineering -> graph building).
3.	Task 10: Documentation & Finalization
o	Sub-Task 10.1: Populate README.md thoroughly:
	Project overview.
	Setup instructions (virtual env, pip install -r requirements.txt).
	How to run data processing, training, and inference.
	Explanation of the chosen GCN/HeteroGCN architecture.
	Assumptions made.
	Potential improvements / future work.
o	Sub-Task 10.2: Ensure all code is well-commented. Add docstrings to all functions and classes.
o	Sub-Task 10.3: Ensure requirements.txt is complete and specifies fixed versions for reproducibility.
VI. TECHNICAL SPECIFICATIONS & METHODOLOGIES
1.	Word Embeddings (BERT/Transformers):
o	Use a pre-trained model from the Hugging Face transformers library (e.g., bert-base-uncased, distilbert-base-uncased for efficiency). Justify your choice.
o	Obtain fixed-size embeddings for job descriptions/titles and professional bios. Concatenate title and description for jobs before embedding, or process them to capture joint meaning.
o	These embeddings will serve as initial node features for Jobs and Professionals in the GCN.
2.	Graph Neural Networks (GCN/HeteroGCN):
o	Library: PyTorch Geometric (PyG) or Deep Graph Library (DGL). State your choice and stick to it. PyG is often more Pythonic if using PyTorch.
o	Rationale: GCNs can learn complex relational patterns. HeteroGCNs are particularly suited because JibJob involves different entity types (Professionals, Jobs, Categories) with different relationships.
o	Implementation Details:
	Define clear node and edge types (especially for HeteroGCN).
	The GCN should learn final embeddings for Professionals and Jobs.
	Recommendation is then performed by finding Jobs whose learned embeddings are closest (e.g., cosine similarity, dot product) to a given Professional's learned embedding.
	Address how the GCN output (node embeddings) is used to make the final recommendation score.
3.	Location Handling:
o	Calculate Haversine distance.
o	Location can be:
	A filter applied post-GCN ranking.
	A feature for Job/Professional nodes (e.g., normalized lat/long, or embedding of location).
	An edge attribute (distance) if a (Professional)-[NEAR_JOB_LOCATION]->(Job) edge is explicitly created.
	A separate node type (Location) with edges (Professional)-[LIVES_AT]->(Location) and (Job)-[IS_AT]->(Location). A meta-path like Prof -(lives_at)-> Loc <-(is_at)- Job could then be learned by a HeteroGCN. This is powerful.
o	Requirement: Recommendations MUST be location-aware. A Professional in City A should not primarily see jobs from City Z unless City Z is very close or other signals are overwhelmingly strong (and configurable distance cutoffs are respected).
VII. CRITICAL CONSIDERATIONS & FLAW AVOIDANCE
You MUST demonstrate you've thought about these issues in your design and implementation, even if full solutions are complex.
1.	Cold Start Problem:
o	New Professionals: How are recommendations generated if a professional has no interaction history (job_applications.csv) and perhaps a sparse profile? (Focus on declared professional_categories, BERT embedding of profile_bio if present, and location).
o	New Jobs: How do new jobs get visibility if no one has applied yet? (Focus on required_category_id, BERT embedding of title/description, location; GCN should learn to match these to professional profiles).
2.	Scalability: While the initial dataset is synthetic, make architectural choices that don't preclude future scaling (e.g., avoid operations with O(N^2) complexity where N is large if alternatives exist, batch processing). Note: Full graph GCNs on very large graphs can be challenging; mention potential for sampling techniques (e.g., GraphSAINT, ClusterGCN) if this were a massive production system.
3.	Data Sparsity: Especially relevant for job_applications.csv. GCNs are generally good at this due to message passing (information propagates from denser parts of the graph). Rich node features (BERT, categories) are crucial.
4.	Bias in Recommendations:
o	Be aware that data can contain biases. For instance, if historical data shows certain demographics only apply to certain job types, the model might perpetuate this. This is a complex ethical issue. For this project, focus on technical accuracy, but briefly acknowledge awareness.
o	Avoid over-promoting "popular" jobs to everyone.
5.	Interpretability/Explainability: GCN recommendations can be hard to explain. While not a primary requirement for this project to implement full explainability (e.g., GNNExplainer), a brief comment in the README on why a job was recommended (e.g., "strong category match," "close proximity," "similar description to jobs you liked") is a plus for future development. The model implicitly uses this; we want to articulate it.
6.	Evaluation Rigor: The definition of "ground truth" for evaluation when interaction data is sparse or unavailable is key. Justify your chosen evaluation protocol and metrics.
7.	Implicit vs. Explicit Signals:
o	Explicit: Professional-selected categories, client-selected required category.
o	Implicit: Text in bio/description, application history.
o	Your system should aim to leverage both. The GCN is excellent for combining various signals.
VIII. DELIVERABLES
1.	Complete Python Codebase: Organized as per Section IV, adhering to all requirements.
2.	requirements.txt file: With pinned versions of all dependencies.
3.	README.md file: Comprehensive documentation.
4.	Configuration File (settings.yaml): With sensible defaults.
5.	(Optional but highly valued): Jupyter notebooks for data exploration and experimentation.
6.	Unit Tests: Demonstrating correctness of key components.
IX. ASSUMPTIONS (State these clearly in your README if you make them)
1.	The provided synthetic data adheres strictly to the schema in Section III.
2.	Sufficient computational resources are available for BERT embedding generation and GCN training (for a moderately sized synthetic dataset).
3.	The primary goal is recommendation accuracy based on job content, professional profile, and location.
4.	English language for all textual data (or state if you're using a multilingual BERT).
FINAL WARNING: This is a complex task. Attention to detail, structured thinking, and proactive problem identification are paramount. Do not take shortcuts that compromise quality or functionality. The "super" in "super structured" is not hyperbole; it's a directive. Deliver accordingly.

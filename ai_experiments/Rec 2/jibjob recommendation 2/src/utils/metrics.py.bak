"""
Evaluation metrics for recommendation systems.
This module provides functions to evaluate the quality of recommendations.
Only includes precision, recall, NDCG, MAE, and RMSE metrics.
"""

import numpy as np
from typing import List, Dict, Tuple, Union, Any
from sklearn.metrics import roc_auc_score, precision_score, recall_score, ndcg_score
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def mean_absolute_error(actual: np.ndarray, predicted: np.ndarray) -> float:
    """
    Calculate Mean Absolute Error (MAE) between actual and predicted ratings.
    
    Args:
        actual: Array of actual ratings.
        predicted: Array of predicted ratings.
        
    Returns:
        float: MAE score.
    """
    if len(actual) == 0:
        return 0.0
    return np.mean(np.abs(actual - predicted))


def root_mean_squared_error(actual: np.ndarray, predicted: np.ndarray) -> float:
    """
    Calculate Root Mean Squared Error (RMSE) between actual and predicted ratings.
    
    Args:
        actual: Array of actual ratings.
        predicted: Array of predicted ratings.
        
    Returns:
        float: RMSE score.
    """
    if len(actual) == 0:
        return 0.0
    return np.sqrt(np.mean((actual - predicted) ** 2))


def precision_at_k(predictions: List[List[Any]], ground_truth: List[List[Any]], k: int) -> float:
    """
    Calculate precision@k for recommendations.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        k: The number of top recommendations to consider.
        
    Returns:
        float: Precision@k score.
    """
    precision_scores = []
    
    for i, pred in enumerate(predictions):
        if i >= len(ground_truth):
            continue
            
        # Consider only the top k predictions
        pred_k = pred[:k]
        
        # Count relevant items in the top k predictions
        relevant_count = sum(1 for item in pred_k if item in ground_truth[i])
        
        # Calculate precision for this user
        if len(pred_k) > 0:
            precision_scores.append(relevant_count / len(pred_k))
        else:
            precision_scores.append(0.0)
    
    # Average precision across users
    return np.mean(precision_scores) if precision_scores else 0.0


def recall_at_k(predictions: List[List[Any]], ground_truth: List[List[Any]], k: int) -> float:
    """
    Calculate recall@k for recommendations.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        k: The number of top recommendations to consider.
        
    Returns:
        float: Recall@k score.
    """
    recall_scores = []
    
    for i, pred in enumerate(predictions):
        if i >= len(ground_truth) or not ground_truth[i]:
            continue
            
        # Consider only the top k predictions
        pred_k = pred[:k]
        
        # Count relevant items in the top k predictions
        relevant_count = sum(1 for item in pred_k if item in ground_truth[i])
        
        # Calculate recall for this user
        recall_scores.append(relevant_count / len(ground_truth[i]))
    
    # Average recall across users
    return np.mean(recall_scores) if recall_scores else 0.0


def ndcg_at_k(predictions: List[List[Any]], ground_truth: List[List[Any]], k: int) -> float:
    """
    Calculate normalized discounted cumulative gain (NDCG) at k.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        k: The number of top recommendations to consider.
        
    Returns:
        float: NDCG@k score.
    """
    ndcg_scores = []
    
    for i, pred in enumerate(predictions):
        if i >= len(ground_truth) or not ground_truth[i]:
            continue
        
        # Consider only the top k predictions
        pred_k = pred[:k]
        
        # Create relevance array (1 if item is relevant, 0 otherwise)
        relevance = np.array([1 if item in ground_truth[i] else 0 for item in pred_k])
        
        # Calculate DCG
        discounts = np.log2(np.arange(2, len(relevance) + 2))  # [log2(2), log2(3), ...]
        dcg = np.sum(relevance / discounts)
        
        # Calculate ideal DCG (IDCG)
        # The ideal is to have all relevant items at the top
        n_rel = min(len(ground_truth[i]), k)
        ideal_relevance = np.ones(n_rel)
        ideal_discounts = np.log2(np.arange(2, n_rel + 2))
        idcg = np.sum(ideal_relevance / ideal_discounts)
        
        # Calculate NDCG
        if idcg > 0:
            ndcg_scores.append(dcg / idcg)
        else:
            ndcg_scores.append(0.0)
    
    # Average NDCG across users
    return np.mean(ndcg_scores) if ndcg_scores else 0.0


def calculate_recommendation_metrics(
    recommendations: List[List[Any]],
    ground_truth: List[List[Any]],
    actual_ratings: np.ndarray = None,
    predicted_ratings: np.ndarray = None,
    k_values: List[int] = [5, 10, 20]
) -> Dict[str, float]:
    """
    Calculate multiple recommendation metrics at different values of k.
    
    Args:
        recommendations: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        actual_ratings: Array of actual ratings (for MAE and RMSE calculation).
        predicted_ratings: Array of predicted ratings (for MAE and RMSE calculation).
        k_values: List of k values to calculate metrics for.
        
    Returns:
        Dict[str, float]: Dictionary with metrics.
    """
    metrics = {}
    
    for k in k_values:
        metrics[f'precision@{k}'] = precision_at_k(recommendations, ground_truth, k)
        metrics[f'recall@{k}'] = recall_at_k(recommendations, ground_truth, k)
        metrics[f'ndcg@{k}'] = ndcg_at_k(recommendations, ground_truth, k)
    
    # Calculate error metrics if rating data is provided
    if actual_ratings is not None and predicted_ratings is not None:
        metrics['mae'] = mean_absolute_error(actual_ratings, predicted_ratings)
        metrics['rmse'] = root_mean_squared_error(actual_ratings, predicted_ratings)
    
    return metrics
        
        # Count relevant items in the top k predictions
        relevant_count = sum(1 for item in pred_k if item in ground_truth[i])
        
        # Calculate recall for this user
        recall_scores.append(relevant_count / len(ground_truth[i]))
    
    # Average recall across users
    return np.mean(recall_scores) if recall_scores else 0.0


def ndcg_at_k(predictions: List[List[Any]], ground_truth: List[List[Any]], k: int) -> float:
    """
    Calculate normalized discounted cumulative gain (NDCG) at k.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        k: The number of top recommendations to consider.
        
    Returns:
        float: NDCG@k score.
    """
    ndcg_scores = []
    
    for i, pred in enumerate(predictions):
        if i >= len(ground_truth) or not ground_truth[i]:
            continue
        
        # Consider only the top k predictions
        pred_k = pred[:k]
        
        # Create relevance array (1 if item is relevant, 0 otherwise)
        relevance = np.array([1 if item in ground_truth[i] else 0 for item in pred_k])
        
        # Calculate DCG
        discounts = np.log2(np.arange(2, len(relevance) + 2))  # [log2(2), log2(3), ...]
        dcg = np.sum(relevance / discounts)
        
        # Calculate ideal DCG (IDCG)
        # The ideal is to have all relevant items at the top
        n_rel = min(len(ground_truth[i]), k)
        ideal_relevance = np.ones(n_rel)
        ideal_discounts = np.log2(np.arange(2, n_rel + 2))
        idcg = np.sum(ideal_relevance / ideal_discounts)
        
        # Calculate NDCG
        if idcg > 0:
            ndcg_scores.append(dcg / idcg)
        else:
            ndcg_scores.append(0.0)
    
    # Average NDCG across users
    return np.mean(ndcg_scores) if ndcg_scores else 0.0


def map_at_k(predictions: List[List[Any]], ground_truth: List[List[Any]], k: int) -> float:
    """
    Calculate Mean Average Precision (MAP) at k.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        k: The number of top recommendations to consider.
        
    Returns:
        float: MAP@k score.
    """
    ap_scores = []
    
    for i, pred in enumerate(predictions):
        if i >= len(ground_truth) or not ground_truth[i]:
            continue
        
        # Consider only the top k predictions
        pred_k = pred[:k]
        
        # Calculate precision at each position with a relevant item
        precisions = []
        relevant_items = 0
        
        for j, item in enumerate(pred_k):
            if item in ground_truth[i]:
                relevant_items += 1
                precisions.append(relevant_items / (j + 1))
        
        # Calculate average precision for this user
        if precisions:
            ap_scores.append(sum(precisions) / min(len(ground_truth[i]), k))
        else:
            ap_scores.append(0.0)
    
    # Calculate mean average precision
    return np.mean(ap_scores) if ap_scores else 0.0


def mrr_at_k(predictions: List[List[Any]], ground_truth: List[List[Any]], k: int) -> float:
    """
    Calculate Mean Reciprocal Rank (MRR) at k.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        k: The number of top recommendations to consider.
        
    Returns:
        float: MRR@k score.
    """
    reciprocal_ranks = []
    
    for i, pred in enumerate(predictions):
        if i >= len(ground_truth) or not ground_truth[i]:
            continue
        
        # Consider only the top k predictions
        pred_k = pred[:k]
        
        # Find the rank of the first relevant item
        rank = None
        for j, item in enumerate(pred_k):
            if item in ground_truth[i]:
                rank = j + 1
                break
        
        # Calculate reciprocal rank
        if rank is not None:
            reciprocal_ranks.append(1.0 / rank)
        else:
            reciprocal_ranks.append(0.0)
    
    # Calculate mean reciprocal rank
    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0


def hit_rate_at_k(predictions: List[List[Any]], ground_truth: List[List[Any]], k: int) -> float:
    """
    Calculate Hit Rate at k.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        k: The number of top recommendations to consider.
        
    Returns:
        float: Hit Rate@k score.
    """
    hit_scores = []
    
    for i, pred in enumerate(predictions):
        if i >= len(ground_truth) or not ground_truth[i]:
            continue
        
        # Consider only the top k predictions
        pred_k = pred[:k]
        
        # Check if at least one relevant item is in the top k predictions
        hit = any(item in ground_truth[i] for item in pred_k)
        hit_scores.append(1.0 if hit else 0.0)
    
    # Calculate hit rate
    return np.mean(hit_scores) if hit_scores else 0.0


def diversity_at_k(predictions: List[List[Any]], k: int) -> float:
    """
    Calculate diversity of recommendations at k.
    Diversity is measured as the average pairwise dissimilarity of items.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        k: The number of top recommendations to consider.
        
    Returns:
        float: Diversity@k score.
    """
    unique_items = set()
    total_recs = 0
    
    for pred in predictions:
        pred_k = pred[:k]
        unique_items.update(pred_k)
        total_recs += len(pred_k)
    
    if total_recs > 0:
        return len(unique_items) / total_recs
    else:
        return 0.0


def coverage(predictions: List[List[Any]], all_items: List[Any]) -> float:
    """
    Calculate catalog coverage of recommendations.
    Coverage is the proportion of items that appear in recommendations.
    
    Args:
        predictions: List of lists where each inner list contains recommended items for a user.
        all_items: List of all items in the catalog.
        
    Returns:
        float: Coverage score.
    """
    recommended_items = set()
    
    for pred in predictions:
        recommended_items.update(pred)
    
    return len(recommended_items) / len(all_items) if all_items else 0.0


def calculate_recommendation_metrics(
    recommendations: List[List[Any]],
    ground_truth: List[List[Any]],
    actual_ratings: np.ndarray = None,
    predicted_ratings: np.ndarray = None,
    k_values: List[int] = [5, 10, 20]
) -> Dict[str, float]:
    """
    Calculate multiple recommendation metrics at different values of k.
    
    Args:
        recommendations: List of lists where each inner list contains recommended items for a user.
        ground_truth: List of lists where each inner list contains relevant items for a user.
        actual_ratings: Array of actual ratings (for MAE and RMSE calculation).
        predicted_ratings: Array of predicted ratings (for MAE and RMSE calculation).
        k_values: List of k values to calculate metrics for.
        
    Returns:
        Dict[str, float]: Dictionary with metrics.
    """
    metrics = {}
    
    for k in k_values:
        metrics[f'precision@{k}'] = precision_at_k(recommendations, ground_truth, k)
        metrics[f'recall@{k}'] = recall_at_k(recommendations, ground_truth, k)
        metrics[f'ndcg@{k}'] = ndcg_at_k(recommendations, ground_truth, k)
    
    # Calculate error metrics if rating data is provided
    if actual_ratings is not None and predicted_ratings is not None:
        metrics['mae'] = mean_absolute_error(actual_ratings, predicted_ratings)
        metrics['rmse'] = root_mean_squared_error(actual_ratings, predicted_ratings)
    
    return metrics
